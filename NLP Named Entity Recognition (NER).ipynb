{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e1114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARSHIT\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\HARSHIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "\n",
    "# Download the CoNLL-2002 dataset\n",
    "nltk.download('conll2002')\n",
    "\n",
    "# Load the dataset\n",
    "data = conll2002.iob_sents()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features for CRF\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'word': word}\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, pos, label in sent]\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = [sent2features(sent) for sent in train_data]\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "X_test = [sent2features(sent) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]\n",
    "\n",
    "# Train a CRF model\n",
    "crf = CRF()\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = crf.predict(X_test)\n",
    "f1_score = flat_f1_score(y_test, y_pred, average='weighted')\n",
    "print(f'Weighted F1 Score: {f1_score}')\n",
    "\n",
    "new_sentence = \"hello navadiya\"\n",
    "\n",
    "# Use the NER model to predict named entities\n",
    "new_prediction = crf.predict([sent2features(new_sentence.split())])\n",
    "\n",
    "print(f'Predicted Entities: {new_prediction[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eebd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
